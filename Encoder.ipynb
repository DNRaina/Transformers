{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "R-r28lyEQlRw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v, mask = None):\n",
        "  dim_k = q.size()[-1]\n",
        "  scaled = torch.matmul(q, k.transpose(-1,-2)) / math.sqrt(dim_k)\n",
        "  print(f\"scaled.size() : {scaled.size()}\")\n",
        "\n",
        "  if mask != None:\n",
        "    print(\"Adding mask - boradcasting add - last N dim must be same\")\n",
        "    scaled +=  mask\n",
        "\n",
        "  attention = F.softmax(scaled, dim = -1)\n",
        "  values = torch.matmul(attention, v)\n",
        "  return values, attention"
      ],
      "metadata": {
        "id": "zK6UXICxzx9w"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, max_sequence_length, d_model = x.size()\n",
        "        print(f\"x.size(): {x.size()}\")\n",
        "        qkv = self.qkv_layer(x)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        qkv = qkv.reshape(batch_size, max_sequence_length, self.num_heads, 3 * self.head_dim)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        print(f\"qkv.size(): {qkv.size()}\")\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n",
        "        values = values.reshape(batch_size, max_sequence_length, self.num_heads * self.head_dim)\n",
        "        print(f\"values.size(): {values.size()}\")\n",
        "        out = self.linear_layer(values)\n",
        "        print(f\"out.size(): {out.size()}\")\n",
        "        return out"
      ],
      "metadata": {
        "id": "7yhKVWDI15er"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self, parameters_shape, eps = 1e-5) -> None:\n",
        "    super().__init__()\n",
        "    self.parameters_shape = parameters_shape\n",
        "    self.eps = eps\n",
        "    self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "    self.beta = nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    dims = [-(i+1) for i in range(len(self.parameters_shape))]\n",
        "    mean = inputs.mean(dim = dims, keepdim = True)\n",
        "    print(f\"mean size = {mean.size()}\")\n",
        "\n",
        "    var = ((inputs - mean)**2).mean(dim = dims, keepdim = True)\n",
        "    std = (var + self.eps).sqrt()\n",
        "    print(f\"Standard deviation : {std}\")\n",
        "\n",
        "    y = (inputs - mean) / std\n",
        "    print(f\"y: {y}\")\n",
        "\n",
        "    out = self.gamma * y + self.beta\n",
        "    print(f\"self.gamma: {self.gamma.size()}, self.beta: {self.beta.size()}\")\n",
        "    print(f\"out: {out.size()}\")\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "18lLWOLeT7DH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "    super(PositionwiseFeedForward, self).__init__()\n",
        "    self.linear1 = nn.Linear(d_model, hidden)\n",
        "    self.linear2 = nn.Linear(hidden, d_model)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    print(f\"x after first linear layer: {x.size()}\")\n",
        "    x = self.relu(x)\n",
        "    print(f\"x after activation: {x.size()}\")\n",
        "    x = self.dropout(x)\n",
        "    print(f\"x after dropout: {x.size()}\")\n",
        "    x = self.linear2(x)\n",
        "    print(f\"x after 2nd linear layer: {x.size()}\")\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "mwei3623Ux4A"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.attention = MultiHeadAttention(d_model = d_model, num_heads = num_heads)\n",
        "    self.norm1 = LayerNormalization(parameters_shape = [d_model])\n",
        "    self.dropout1 = nn.Dropout(p = drop_prob)\n",
        "    self.ffn = PositionwiseFeedForward(d_model = d_model, hidden = ffn_hidden, drop_prob = drop_prob)\n",
        "    self.norm2 = LayerNormalization(parameters_shape = [d_model])\n",
        "    self.dropout2 = nn.Dropout(p = drop_prob)\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual_x = x\n",
        "    print(\"Attention 1 ----------\")\n",
        "    x = self.attention(x, mask = None)\n",
        "    print(\"Dropout 1 ---------- \")\n",
        "    x = self.dropout1(x)\n",
        "    print(\"add and layer normalization 1 ----------\")\n",
        "    x = self.norm1(x+residual_x)\n",
        "    residual_x = x\n",
        "    print(\"attention 2 ----------\")\n",
        "    x = self.ffn(x)\n",
        "    print(\"dropout 2 ----------\")\n",
        "    x = self.dropout2(x)\n",
        "    print(\"add and norm 2 ----------\")\n",
        "    x = self.norm2(x +residual_x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "5RYYbANsX1EV"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList(\n",
        "            [EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ufJKxpYFZrF6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "batch_size = 32\n",
        "max_sequence_length = 200\n",
        "ffn_hidden = 2048\n",
        "num_layers = 6\n",
        "\n",
        "encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)"
      ],
      "metadata": {
        "id": "EVyDJtPcZzoc"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn( (batch_size, max_sequence_length, d_model) ) # includes positional encoding\n",
        "out = encoder(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7MI9qOJaxaN",
        "outputId": "81005508-6db1-452c-df2e-801e5601f514"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention 1 ----------\n",
            "x.size(): torch.Size([32, 200, 512])\n",
            "qkv.size(): torch.Size([32, 200, 1536])\n",
            "qkv.size(): torch.Size([32, 200, 8, 192])\n",
            "qkv.size(): torch.Size([32, 8, 200, 192])\n",
            "q size: torch.Size([32, 8, 200, 64]), k size: torch.Size([32, 8, 200, 64]), v size: torch.Size([32, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([32, 8, 200, 200])\n",
            "values.size(): torch.Size([32, 8, 200, 64]), attention.size:torch.Size([32, 8, 200, 200]) \n",
            "values.size(): torch.Size([32, 200, 512])\n",
            "out.size(): torch.Size([32, 200, 512])\n",
            "Dropout 1 ---------- \n",
            "add and layer normalization 1 ----------\n",
            "mean size = torch.Size([32, 200, 1])\n",
            "Standard deviation : tensor([[[0.9631],\n",
            "         [0.9738],\n",
            "         [1.0439],\n",
            "         ...,\n",
            "         [1.0037],\n",
            "         [0.9882],\n",
            "         [0.9767]],\n",
            "\n",
            "        [[1.0033],\n",
            "         [1.0077],\n",
            "         [0.9886],\n",
            "         ...,\n",
            "         [0.9464],\n",
            "         [1.0212],\n",
            "         [1.0458]],\n",
            "\n",
            "        [[1.0211],\n",
            "         [1.0036],\n",
            "         [1.0193],\n",
            "         ...,\n",
            "         [0.9524],\n",
            "         [0.9846],\n",
            "         [0.9936]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.0775],\n",
            "         [1.0133],\n",
            "         [0.9622],\n",
            "         ...,\n",
            "         [0.9827],\n",
            "         [0.9601],\n",
            "         [0.9521]],\n",
            "\n",
            "        [[0.9202],\n",
            "         [1.0468],\n",
            "         [0.9601],\n",
            "         ...,\n",
            "         [0.9770],\n",
            "         [1.0037],\n",
            "         [0.9757]],\n",
            "\n",
            "        [[0.9915],\n",
            "         [0.9738],\n",
            "         [1.0300],\n",
            "         ...,\n",
            "         [1.0043],\n",
            "         [1.0213],\n",
            "         [1.0129]]], grad_fn=<SqrtBackward0>)\n",
            "y: tensor([[[ 0.0412, -0.6323, -0.5513,  ..., -0.7218,  1.8777,  0.4765],\n",
            "         [ 1.1662, -0.6903,  1.5918,  ...,  2.0132,  0.6338, -0.0927],\n",
            "         [ 0.7632, -0.1621,  0.7654,  ...,  1.5243, -0.6840, -0.2707],\n",
            "         ...,\n",
            "         [-0.5778,  1.8351,  0.2984,  ...,  1.4276,  0.2238, -0.3926],\n",
            "         [-0.7362, -1.0168,  0.3471,  ...,  0.1646,  0.7046,  0.9623],\n",
            "         [ 0.1427, -0.6388, -0.2351,  ...,  0.1368, -0.3929, -0.4887]],\n",
            "\n",
            "        [[ 0.2773, -0.0906,  0.2861,  ..., -0.9048, -1.4564, -0.8446],\n",
            "         [ 0.2693,  0.3895, -0.3921,  ..., -1.6668,  1.6977,  1.6335],\n",
            "         [ 2.3715, -0.4078,  0.3465,  ..., -0.3508,  1.6399, -0.2509],\n",
            "         ...,\n",
            "         [-0.1792,  1.2203, -0.7775,  ...,  0.6606,  2.6191,  1.2200],\n",
            "         [ 0.2613, -2.6159, -0.5469,  ...,  2.0920,  0.1454,  1.1364],\n",
            "         [-1.3795, -0.5118,  2.0397,  ..., -1.0179,  0.0670, -0.2817]],\n",
            "\n",
            "        [[-0.9927, -0.3640,  0.7015,  ...,  2.5864,  0.9287,  1.5491],\n",
            "         [ 0.8582,  0.0450, -1.6350,  ...,  0.2070, -0.0030,  0.8347],\n",
            "         [-0.6547,  0.7147, -1.1502,  ..., -0.3535,  0.7535, -0.7876],\n",
            "         ...,\n",
            "         [-0.5633, -0.5706, -1.0957,  ...,  0.5940, -0.8276,  0.1535],\n",
            "         [-0.6559,  1.5009,  0.4859,  ..., -0.4035,  0.9582, -1.3404],\n",
            "         [-0.7376,  0.2533,  0.2348,  ..., -0.1961, -0.7956,  1.4660]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.9439,  1.5582,  1.0352,  ...,  1.4954,  0.1892, -1.2711],\n",
            "         [ 1.3353,  0.8662,  0.3759,  ...,  0.5289, -0.4511,  0.4984],\n",
            "         [ 0.2349,  1.4146,  0.0424,  ...,  0.1400, -0.5617, -0.3050],\n",
            "         ...,\n",
            "         [ 1.5992,  1.1416, -0.0953,  ...,  0.5374,  1.4759,  0.6437],\n",
            "         [ 0.1305, -0.1948,  0.2943,  ...,  0.3786,  0.8880,  1.6517],\n",
            "         [ 0.8310, -0.7467, -0.1167,  ...,  0.5533,  0.4951, -0.5796]],\n",
            "\n",
            "        [[ 1.0288,  0.4091,  1.8996,  ...,  0.2630, -1.4817, -0.5867],\n",
            "         [-0.7079, -0.3978,  0.1283,  ...,  0.1400, -0.5144, -0.3696],\n",
            "         [-1.3212,  0.0901, -1.1311,  ..., -0.0962,  0.3532,  0.3058],\n",
            "         ...,\n",
            "         [-0.9359, -1.2051, -0.5285,  ..., -1.0529, -1.3661,  1.3559],\n",
            "         [-0.0552,  0.1125,  0.5251,  ..., -2.1312, -0.0551,  0.5367],\n",
            "         [-2.0505,  1.3471, -0.4634,  ...,  0.7692, -1.1865,  1.0370]],\n",
            "\n",
            "        [[ 1.6789,  0.7030, -0.0872,  ...,  1.1851,  0.4268,  0.5130],\n",
            "         [ 2.1408,  0.7175, -0.4749,  ...,  2.5023,  1.5518,  0.8148],\n",
            "         [ 0.8761, -0.6220,  0.0799,  ..., -0.6187, -1.1040, -0.4038],\n",
            "         ...,\n",
            "         [-0.0678, -1.5428, -0.1983,  ..., -0.5633,  0.3277, -1.3466],\n",
            "         [ 0.3716, -1.6698,  1.2356,  ...,  0.1453,  0.5721, -2.1602],\n",
            "         [ 1.0829, -0.8986,  0.4734,  ...,  1.6955,  0.3341,  1.3922]]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([32, 200, 512])\n",
            "attention 2 ----------\n",
            "x after first linear layer: torch.Size([32, 200, 2048])\n",
            "x after activation: torch.Size([32, 200, 2048])\n",
            "x after dropout: torch.Size([32, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([32, 200, 512])\n",
            "dropout 2 ----------\n",
            "add and norm 2 ----------\n",
            "mean size = torch.Size([32, 200, 1])\n",
            "Standard deviation : tensor([[[1.0348],\n",
            "         [1.0279],\n",
            "         [1.0302],\n",
            "         ...,\n",
            "         [1.0318],\n",
            "         [1.0212],\n",
            "         [1.0399]],\n",
            "\n",
            "        [[1.0339],\n",
            "         [1.0553],\n",
            "         [1.0280],\n",
            "         ...,\n",
            "         [1.0458],\n",
            "         [1.0410],\n",
            "         [1.0514]],\n",
            "\n",
            "        [[1.0342],\n",
            "         [1.0445],\n",
            "         [1.0198],\n",
            "         ...,\n",
            "         [1.0440],\n",
            "         [1.0466],\n",
            "         [1.0338]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.0334],\n",
            "         [1.0325],\n",
            "         [1.0261],\n",
            "         ...,\n",
            "         [1.0371],\n",
            "         [1.0449],\n",
            "         [1.0522]],\n",
            "\n",
            "        [[1.0527],\n",
            "         [1.0448],\n",
            "         [1.0292],\n",
            "         ...,\n",
            "         [1.0094],\n",
            "         [1.0338],\n",
            "         [1.0432]],\n",
            "\n",
            "        [[1.0340],\n",
            "         [1.0406],\n",
            "         [1.0288],\n",
            "         ...,\n",
            "         [1.0382],\n",
            "         [1.0376],\n",
            "         [1.0367]]], grad_fn=<SqrtBackward0>)\n",
            "y: tensor([[[ 0.5537, -0.3170,  0.2776,  ..., -0.6870,  1.4710,  0.8818],\n",
            "         [ 1.2854, -0.6701,  1.6306,  ...,  2.4808,  0.0960, -0.0235],\n",
            "         [ 1.1779, -0.5221,  0.3129,  ...,  1.5592, -0.9993, -0.3288],\n",
            "         ...,\n",
            "         [ 0.0184,  2.0224,  0.0444,  ...,  1.5111,  0.2660, -0.3300],\n",
            "         [-0.2051, -1.1681,  0.5159,  ...,  0.5997,  0.0943,  0.9378],\n",
            "         [ 0.7950, -0.9740, -0.2346,  ..., -0.1050, -0.3388, -0.4800]],\n",
            "\n",
            "        [[ 0.2791, -0.1557,  0.3711,  ..., -0.4786, -1.0725, -1.1209],\n",
            "         [ 0.4412,  0.3642, -0.3764,  ..., -1.5490,  1.4746,  1.5874],\n",
            "         [ 2.8419, -0.6801,  0.2190,  ..., -0.2484,  2.0437, -0.2233],\n",
            "         ...,\n",
            "         [ 0.1741,  1.3021, -0.5697,  ...,  0.6112,  2.0540,  0.8766],\n",
            "         [ 0.5662, -2.6219, -0.0553,  ...,  2.2014, -0.2525,  0.8796],\n",
            "         [-1.2398, -1.1927,  1.9987,  ..., -1.0081, -0.1242, -0.2796]],\n",
            "\n",
            "        [[-0.5971, -0.4643,  0.8668,  ...,  2.7849,  0.6194,  1.7319],\n",
            "         [ 1.0864, -0.3391, -1.4388,  ...,  0.2370,  0.0256,  0.8711],\n",
            "         [-0.2436,  0.8275, -0.7412,  ..., -0.1451,  0.8185, -0.7953],\n",
            "         ...,\n",
            "         [-0.2558, -0.8946, -0.7045,  ...,  0.4521, -0.6084,  0.4943],\n",
            "         [-0.1583,  1.6541,  0.5782,  ..., -0.3014,  1.2453, -1.0729],\n",
            "         [-0.2814, -0.0864,  0.2334,  ..., -0.2759, -0.6994,  1.4840]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.6099,  1.4992,  1.0900,  ...,  1.4767,  0.0758, -1.2157],\n",
            "         [ 1.4479,  0.8401,  0.3652,  ...,  0.5730, -0.5031,  0.4838],\n",
            "         [ 0.4671,  1.2270, -0.0464,  ...,  0.0342, -0.5477, -0.2587],\n",
            "         ...,\n",
            "         [ 1.6593,  0.8784,  0.0483,  ...,  1.2095,  1.4328,  0.4816],\n",
            "         [ 0.3620,  0.1151,  0.3244,  ...,  0.0794,  1.0921,  1.8112],\n",
            "         [ 1.5446, -0.7268, -0.1466,  ...,  0.5710,  0.7136, -0.7549]],\n",
            "\n",
            "        [[ 1.0358,  0.3828,  2.2632,  ..., -0.0528, -1.3639, -0.7296],\n",
            "         [-0.7505, -0.6191,  0.3912,  ...,  0.1241, -0.3881, -0.0245],\n",
            "         [-0.7700,  0.0212, -0.7635,  ...,  0.0767, -0.3060,  0.3002],\n",
            "         ...,\n",
            "         [-0.7992, -1.8483, -0.0132,  ..., -0.9042, -1.3882,  1.5517],\n",
            "         [-0.0629, -0.2055,  0.6516,  ..., -1.7446, -0.0339,  0.6758],\n",
            "         [-1.6755,  1.0154, -0.4606,  ...,  0.8504, -1.5342,  1.2076]],\n",
            "\n",
            "        [[ 1.7474,  0.2050,  0.4578,  ...,  1.1535,  0.2672,  0.6461],\n",
            "         [ 2.0596,  0.6095, -0.0458,  ...,  2.7378,  1.4726,  0.7807],\n",
            "         [ 1.5094, -0.6093,  0.0590,  ..., -0.5356, -0.7282, -0.1187],\n",
            "         ...,\n",
            "         [-0.0723, -1.5749, -0.4273,  ..., -0.5742,  0.0353, -1.1098],\n",
            "         [ 0.6635, -1.6230,  1.1917,  ...,  0.2821,  0.2122, -1.9791],\n",
            "         [ 1.2857, -1.1598,  0.7841,  ...,  1.7774,  0.4899,  1.4219]]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([32, 200, 512])\n",
            "Attention 1 ----------\n",
            "x.size(): torch.Size([32, 200, 512])\n",
            "qkv.size(): torch.Size([32, 200, 1536])\n",
            "qkv.size(): torch.Size([32, 200, 8, 192])\n",
            "qkv.size(): torch.Size([32, 8, 200, 192])\n",
            "q size: torch.Size([32, 8, 200, 64]), k size: torch.Size([32, 8, 200, 64]), v size: torch.Size([32, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([32, 8, 200, 200])\n",
            "values.size(): torch.Size([32, 8, 200, 64]), attention.size:torch.Size([32, 8, 200, 200]) \n",
            "values.size(): torch.Size([32, 200, 512])\n",
            "out.size(): torch.Size([32, 200, 512])\n",
            "Dropout 1 ---------- \n",
            "add and layer normalization 1 ----------\n",
            "mean size = torch.Size([32, 200, 1])\n",
            "Standard deviation : tensor([[[1.0004],\n",
            "         [0.9958],\n",
            "         [0.9972],\n",
            "         ...,\n",
            "         [0.9964],\n",
            "         [0.9998],\n",
            "         [1.0023]],\n",
            "\n",
            "        [[1.0034],\n",
            "         [1.0009],\n",
            "         [1.0016],\n",
            "         ...,\n",
            "         [0.9988],\n",
            "         [0.9994],\n",
            "         [0.9991]],\n",
            "\n",
            "        [[1.0013],\n",
            "         [0.9995],\n",
            "         [1.0001],\n",
            "         ...,\n",
            "         [0.9980],\n",
            "         [1.0013],\n",
            "         [0.9985]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.0034],\n",
            "         [0.9990],\n",
            "         [1.0043],\n",
            "         ...,\n",
            "         [1.0054],\n",
            "         [1.0017],\n",
            "         [0.9994]],\n",
            "\n",
            "        [[1.0039],\n",
            "         [1.0023],\n",
            "         [1.0008],\n",
            "         ...,\n",
            "         [1.0056],\n",
            "         [1.0011],\n",
            "         [1.0033]],\n",
            "\n",
            "        [[1.0007],\n",
            "         [0.9991],\n",
            "         [1.0045],\n",
            "         ...,\n",
            "         [1.0025],\n",
            "         [1.0036],\n",
            "         [0.9998]]], grad_fn=<SqrtBackward0>)\n",
            "y: tensor([[[ 5.2377e-01, -4.6419e-01,  3.4640e-01,  ..., -7.2241e-01,\n",
            "           1.5250e+00,  9.8920e-01],\n",
            "         [ 1.2839e+00, -8.2643e-01,  1.7383e+00,  ...,  2.4361e+00,\n",
            "           1.6456e-01,  1.1130e-01],\n",
            "         [ 1.1498e+00, -6.8431e-01,  3.9627e-01,  ...,  1.5414e+00,\n",
            "          -1.0034e+00, -2.3005e-01],\n",
            "         ...,\n",
            "         [ 2.8095e-02,  1.9576e+00,  8.6044e-02,  ...,  1.5055e+00,\n",
            "           2.3162e-01, -3.3878e-01],\n",
            "         [-1.8799e-01, -1.2302e+00,  5.5526e-01,  ...,  5.9479e-01,\n",
            "           9.2253e-02,  9.2533e-01],\n",
            "         [ 8.0966e-01, -1.0522e+00, -1.9353e-01,  ..., -9.3316e-02,\n",
            "          -3.6086e-01, -4.8383e-01]],\n",
            "\n",
            "        [[ 2.6036e-01, -2.6942e-01,  4.3216e-01,  ..., -5.3669e-01,\n",
            "          -1.0340e+00, -9.7311e-01],\n",
            "         [ 4.4331e-01,  2.4247e-01, -3.1073e-01,  ..., -1.6201e+00,\n",
            "           1.5211e+00,  1.7165e+00],\n",
            "         [ 2.8282e+00, -7.9847e-01,  2.7225e-01,  ..., -3.1075e-01,\n",
            "           2.0957e+00, -1.0801e-01],\n",
            "         ...,\n",
            "         [ 1.7293e-01,  1.2802e+00, -5.3717e-01,  ...,  6.1665e-01,\n",
            "           2.0144e+00,  8.5514e-01],\n",
            "         [ 5.8673e-01, -2.6707e+00,  2.3484e-03,  ...,  2.1666e+00,\n",
            "          -2.9590e-01,  8.4679e-01],\n",
            "         [-1.2241e+00, -1.2231e+00,  2.0301e+00,  ..., -1.0427e+00,\n",
            "          -1.4822e-01, -3.1216e-01]],\n",
            "\n",
            "        [[-5.8908e-01, -5.6275e-01,  9.1839e-01,  ...,  2.7861e+00,\n",
            "           6.8619e-01,  1.8628e+00],\n",
            "         [ 1.0748e+00, -3.4031e-01, -1.3818e+00,  ...,  2.6695e-01,\n",
            "           1.1718e-01,  8.7042e-01],\n",
            "         [-2.5174e-01,  7.2833e-01, -6.8939e-01,  ..., -1.3907e-01,\n",
            "           9.0554e-01, -6.6699e-01],\n",
            "         ...,\n",
            "         [-2.5875e-01, -9.7051e-01, -6.4136e-01,  ...,  4.3461e-01,\n",
            "          -6.4867e-01,  5.3346e-01],\n",
            "         [-1.6152e-01,  1.6493e+00,  6.5032e-01,  ..., -2.9956e-01,\n",
            "           1.2323e+00, -1.0741e+00],\n",
            "         [-2.8281e-01, -1.7658e-01,  2.8325e-01,  ..., -2.8048e-01,\n",
            "          -6.9684e-01,  1.4923e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-6.2182e-01,  1.4246e+00,  1.1313e+00,  ...,  1.4812e+00,\n",
            "           1.1493e-01, -1.0917e+00],\n",
            "         [ 1.4272e+00,  7.8795e-01,  4.1715e-01,  ...,  5.8434e-01,\n",
            "          -4.4883e-01,  6.0097e-01],\n",
            "         [ 4.5112e-01,  1.2190e+00, -1.0724e-02,  ...,  3.1324e-02,\n",
            "          -5.0151e-01, -1.3723e-01],\n",
            "         ...,\n",
            "         [ 1.6629e+00,  8.6066e-01,  1.0753e-01,  ...,  1.2357e+00,\n",
            "           1.4273e+00,  5.0024e-01],\n",
            "         [ 3.5796e-01,  9.6841e-02,  3.8008e-01,  ...,  1.1377e-01,\n",
            "           1.0869e+00,  1.8242e+00],\n",
            "         [ 1.5701e+00, -7.5769e-01, -9.0608e-02,  ...,  5.9145e-01,\n",
            "           7.1093e-01, -7.5101e-01]],\n",
            "\n",
            "        [[ 1.0351e+00,  2.3510e-01,  2.3103e+00,  ..., -5.2549e-02,\n",
            "          -1.3153e+00, -6.3465e-01],\n",
            "         [-7.3970e-01, -7.5504e-01,  4.4058e-01,  ...,  1.1565e-01,\n",
            "          -3.2714e-01,  6.6150e-02],\n",
            "         [-7.5691e-01, -1.1358e-01, -7.1000e-01,  ...,  4.1046e-02,\n",
            "          -2.5141e-01,  3.9160e-01],\n",
            "         ...,\n",
            "         [-7.9473e-01, -1.8452e+00,  8.1938e-02,  ..., -8.9913e-01,\n",
            "          -1.3505e+00,  1.5638e+00],\n",
            "         [-5.7352e-02, -2.0843e-01,  7.4279e-01,  ..., -1.7034e+00,\n",
            "          -3.7010e-02,  7.0619e-01],\n",
            "         [-1.6570e+00,  9.9698e-01, -4.5807e-01,  ...,  8.7575e-01,\n",
            "          -1.5082e+00,  1.2214e+00]],\n",
            "\n",
            "        [[ 1.7973e+00,  3.4547e-02,  5.2149e-01,  ...,  1.1615e+00,\n",
            "           3.8445e-01,  7.6872e-01],\n",
            "         [ 2.0875e+00,  4.6427e-01,  2.5076e-02,  ...,  2.7463e+00,\n",
            "           1.5960e+00,  8.7437e-01],\n",
            "         [ 1.5341e+00, -7.5011e-01,  1.3055e-01,  ..., -5.2079e-01,\n",
            "          -6.1793e-01, -8.8844e-03],\n",
            "         ...,\n",
            "         [-1.1581e-01, -1.6383e+00, -4.2729e-01,  ..., -5.9900e-01,\n",
            "           4.8858e-02, -1.0992e+00],\n",
            "         [ 6.5868e-01, -1.6703e+00,  1.2493e+00,  ...,  2.2071e-01,\n",
            "           2.2567e-01, -1.9669e+00],\n",
            "         [ 1.2633e+00, -1.2284e+00,  8.5122e-01,  ...,  1.7758e+00,\n",
            "           4.9723e-01,  1.4456e+00]]], grad_fn=<DivBackward0>)\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([32, 200, 512])\n",
            "attention 2 ----------\n",
            "x after first linear layer: torch.Size([32, 200, 2048])\n",
            "x after activation: torch.Size([32, 200, 2048])\n",
            "x after dropout: torch.Size([32, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([32, 200, 512])\n",
            "dropout 2 ----------\n",
            "add and norm 2 ----------\n",
            "mean size = torch.Size([32, 200, 1])\n",
            "Standard deviation : tensor([[[1.0267],\n",
            "         [1.0456],\n",
            "         [1.0190],\n",
            "         ...,\n",
            "         [1.0581],\n",
            "         [1.0495],\n",
            "         [1.0203]],\n",
            "\n",
            "        [[1.0373],\n",
            "         [1.0326],\n",
            "         [1.0425],\n",
            "         ...,\n",
            "         [1.0391],\n",
            "         [1.0366],\n",
            "         [1.0354]],\n",
            "\n",
            "        [[1.0362],\n",
            "         [1.0473],\n",
            "         [1.0428],\n",
            "         ...,\n",
            "         [1.0468],\n",
            "         [1.0486],\n",
            "         [1.0254]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.0197],\n",
            "         [1.0400],\n",
            "         [1.0417],\n",
            "         ...,\n",
            "         [1.0301],\n",
            "         [1.0390],\n",
            "         [1.0261]],\n",
            "\n",
            "        [[1.0440],\n",
            "         [1.0500],\n",
            "         [1.0268],\n",
            "         ...,\n",
            "         [1.0299],\n",
            "         [1.0609],\n",
            "         [1.0460]],\n",
            "\n",
            "        [[1.0242],\n",
            "         [1.0263],\n",
            "         [1.0344],\n",
            "         ...,\n",
            "         [1.0293],\n",
            "         [1.0275],\n",
            "         [1.0262]]], grad_fn=<SqrtBackward0>)\n",
            "y: tensor([[[ 0.2393, -0.6498,  0.1962,  ..., -0.7029,  1.5335,  0.7536],\n",
            "         [ 1.2410, -0.5108,  2.0536,  ...,  2.3327,  0.2021,  0.1370],\n",
            "         [ 1.2297, -0.5787,  0.3931,  ...,  1.4016, -0.7160, -0.2134],\n",
            "         ...,\n",
            "         [-0.0133,  2.0233,  0.5431,  ...,  1.8570,  0.1883, -0.3117],\n",
            "         [-0.4704, -0.5909,  0.9305,  ...,  0.2631, -0.4575,  0.7168],\n",
            "         [ 0.9556, -0.8726,  0.2955,  ..., -0.1905, -0.6893, -0.4041]],\n",
            "\n",
            "        [[ 0.2644,  0.0729,  0.9189,  ..., -0.5836, -1.0704, -0.7013],\n",
            "         [ 0.2431,  0.4632, -0.1869,  ..., -0.9776,  1.8011,  1.7951],\n",
            "         [ 2.6189, -0.9758,  0.6421,  ...,  0.3180,  1.9935, -0.2615],\n",
            "         ...,\n",
            "         [ 0.2886,  1.3496, -0.5019,  ...,  0.6084,  1.7404,  1.0427],\n",
            "         [ 0.4368, -2.3133,  0.1338,  ...,  1.7793, -0.3295,  0.8027],\n",
            "         [-1.4465, -1.1263,  1.8585,  ..., -1.1017, -0.0695, -0.0335]],\n",
            "\n",
            "        [[-0.5942, -0.7229,  1.3699,  ...,  2.7800,  0.5619,  2.0764],\n",
            "         [ 0.7570, -0.3537, -0.7772,  ...,  0.3809,  0.1482,  1.0861],\n",
            "         [-0.1766,  0.7979, -0.1546,  ..., -0.0429,  1.1251, -0.4445],\n",
            "         ...,\n",
            "         [-0.6689, -0.9242, -0.5089,  ...,  0.4274, -0.7478,  0.5685],\n",
            "         [-0.3974,  1.9520,  0.8924,  ..., -0.1932,  1.4998, -0.7845],\n",
            "         [-0.4072, -0.2393, -0.0743,  ..., -0.2062, -1.0799,  1.7084]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.7129,  1.4114,  1.3794,  ...,  1.5979,  0.1081, -1.0002],\n",
            "         [ 1.1460,  0.7938,  0.4629,  ...,  0.6130, -0.3565,  0.9829],\n",
            "         [ 0.4371,  1.4066,  0.0096,  ..., -0.1380, -0.6991,  0.1126],\n",
            "         ...,\n",
            "         [ 1.7485,  0.9871,  0.3400,  ...,  1.0076,  1.0604,  0.5678],\n",
            "         [ 0.3520,  0.3104,  0.7052,  ...,  0.5717,  1.4574,  2.2242],\n",
            "         [ 1.0652, -0.4533,  0.0980,  ...,  0.2467,  0.1691, -0.4368]],\n",
            "\n",
            "        [[ 1.0154,  0.5466,  2.1249,  ..., -0.0263, -1.4235, -0.4565],\n",
            "         [-0.6603, -0.7647,  0.3937,  ...,  0.2915, -0.3681,  0.2176],\n",
            "         [-0.9411, -0.3865, -0.6526,  ..., -0.0941, -0.7511,  0.1665],\n",
            "         ...,\n",
            "         [-1.0942, -1.6192,  0.1735,  ..., -0.4870, -1.4357,  1.7208],\n",
            "         [ 0.1379, -0.1808,  1.0117,  ..., -1.4840, -0.3644,  0.8004],\n",
            "         [-1.5165,  0.7594, -0.4250,  ...,  0.7922, -1.3824,  1.1465]],\n",
            "\n",
            "        [[ 1.3929,  0.0304,  1.1180,  ...,  1.1942,  0.5494,  0.8063],\n",
            "         [ 2.2239,  0.6671,  0.4871,  ...,  2.8981,  1.5546,  1.1964],\n",
            "         [ 1.2997, -0.5430,  0.7533,  ..., -0.4908, -0.5087,  0.0040],\n",
            "         ...,\n",
            "         [-0.3328, -1.5857, -0.1359,  ..., -0.7702,  0.0563, -0.9460],\n",
            "         [ 0.7969, -1.3206,  1.3656,  ...,  0.4783,  0.1250, -1.7576],\n",
            "         [ 0.7193, -1.1181,  0.8536,  ...,  1.7545,  0.4121,  1.5817]]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([32, 200, 512])\n",
            "Attention 1 ----------\n",
            "x.size(): torch.Size([32, 200, 512])\n",
            "qkv.size(): torch.Size([32, 200, 1536])\n",
            "qkv.size(): torch.Size([32, 200, 8, 192])\n",
            "qkv.size(): torch.Size([32, 8, 200, 192])\n",
            "q size: torch.Size([32, 8, 200, 64]), k size: torch.Size([32, 8, 200, 64]), v size: torch.Size([32, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([32, 8, 200, 200])\n",
            "values.size(): torch.Size([32, 8, 200, 64]), attention.size:torch.Size([32, 8, 200, 200]) \n",
            "values.size(): torch.Size([32, 200, 512])\n",
            "out.size(): torch.Size([32, 200, 512])\n",
            "Dropout 1 ---------- \n",
            "add and layer normalization 1 ----------\n",
            "mean size = torch.Size([32, 200, 1])\n",
            "Standard deviation : tensor([[[1.0014],\n",
            "         [1.0043],\n",
            "         [1.0084],\n",
            "         ...,\n",
            "         [0.9996],\n",
            "         [0.9984],\n",
            "         [0.9988]],\n",
            "\n",
            "        [[1.0034],\n",
            "         [1.0060],\n",
            "         [1.0034],\n",
            "         ...,\n",
            "         [1.0064],\n",
            "         [1.0082],\n",
            "         [0.9978]],\n",
            "\n",
            "        [[1.0084],\n",
            "         [1.0091],\n",
            "         [1.0039],\n",
            "         ...,\n",
            "         [1.0078],\n",
            "         [1.0014],\n",
            "         [1.0039]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.0009],\n",
            "         [1.0073],\n",
            "         [1.0050],\n",
            "         ...,\n",
            "         [1.0038],\n",
            "         [1.0079],\n",
            "         [1.0076]],\n",
            "\n",
            "        [[1.0016],\n",
            "         [1.0059],\n",
            "         [1.0053],\n",
            "         ...,\n",
            "         [1.0067],\n",
            "         [1.0031],\n",
            "         [1.0042]],\n",
            "\n",
            "        [[1.0041],\n",
            "         [1.0049],\n",
            "         [0.9973],\n",
            "         ...,\n",
            "         [1.0029],\n",
            "         [1.0027],\n",
            "         [1.0050]]], grad_fn=<SqrtBackward0>)\n",
            "y: tensor([[[ 0.2826, -0.6304,  0.1431,  ..., -0.7438,  1.5315,  0.8300],\n",
            "         [ 1.3029, -0.5125,  1.9547,  ...,  2.2752,  0.2075,  0.2090],\n",
            "         [ 1.2871, -0.6054,  0.3181,  ...,  1.3260, -0.7194, -0.1125],\n",
            "         ...,\n",
            "         [-0.0459,  2.0214,  0.5158,  ...,  1.8132,  0.2859, -0.2776],\n",
            "         [-0.5120, -0.5763,  0.9153,  ...,  0.2036, -0.3806,  0.7613],\n",
            "         [ 0.9233, -0.8609,  0.2809,  ..., -0.2377, -0.6085, -0.4110]],\n",
            "\n",
            "        [[ 0.2804,  0.0535,  0.9057,  ..., -0.6548, -1.0758, -0.5513],\n",
            "         [ 0.2732,  0.4614, -0.2207,  ..., -0.9708,  1.7913,  1.9383],\n",
            "         [ 2.6654, -0.9738,  0.6414,  ...,  0.3185,  1.9522, -0.1137],\n",
            "         ...,\n",
            "         [ 0.2843,  1.3227, -0.5396,  ...,  0.6330,  1.8408,  1.0912],\n",
            "         [ 0.4433, -2.2994,  0.1029,  ...,  1.8019, -0.2142,  0.8396],\n",
            "         [-1.4612, -1.1531,  1.8344,  ..., -1.0476,  0.0580, -0.0373]],\n",
            "\n",
            "        [[-0.5759, -0.6716,  1.3231,  ...,  2.7586,  0.5582,  2.2215],\n",
            "         [ 0.7419, -0.3189, -0.8207,  ...,  0.3717,  0.1484,  1.2350],\n",
            "         [-0.1880,  0.8477, -0.1793,  ..., -0.0416,  1.1293, -0.2810],\n",
            "         ...,\n",
            "         [-0.7147, -0.9065, -0.5522,  ...,  0.3812, -0.6416,  0.5441],\n",
            "         [-0.4317,  1.9756,  0.8417,  ..., -0.2291,  1.6019, -0.7978],\n",
            "         [-0.4394, -0.2430, -0.1105,  ..., -0.2282, -0.9665,  1.6954]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.6888,  1.4634,  1.3278,  ...,  1.5616,  0.1082, -0.8880],\n",
            "         [ 1.1802,  0.8512,  0.3822,  ...,  0.5733, -0.3610,  1.1042],\n",
            "         [ 0.4754,  1.4533, -0.0410,  ..., -0.1831, -0.7277,  0.2134],\n",
            "         ...,\n",
            "         [ 1.7418,  1.0207,  0.2715,  ...,  1.0549,  1.1852,  0.5818],\n",
            "         [ 0.3540,  0.3530,  0.6156,  ...,  0.6352,  1.5799,  2.2265],\n",
            "         [ 1.0570, -0.3962,  0.0600,  ...,  0.3039,  0.2892, -0.4080]],\n",
            "\n",
            "        [[ 1.0187,  0.6000,  2.0719,  ..., -0.0418, -1.4188, -0.4534],\n",
            "         [-0.6531, -0.7134,  0.3948,  ...,  0.2789, -0.3227,  0.3810],\n",
            "         [-0.9567, -0.3253, -0.7014,  ..., -0.0944, -0.7080,  0.3161],\n",
            "         ...,\n",
            "         [-1.0973, -1.6117,  0.1186,  ..., -0.4896, -1.3107,  1.7035],\n",
            "         [ 0.1458, -0.2043,  0.9686,  ..., -1.5336, -0.2361,  0.7733],\n",
            "         [-1.5172,  0.7305, -0.4303,  ...,  0.7282, -1.2566,  1.1253]],\n",
            "\n",
            "        [[ 1.4013,  0.0308,  1.0211,  ...,  1.1285,  0.5634,  0.8041],\n",
            "         [ 2.2383,  0.6653,  0.4104,  ...,  2.8336,  1.5726,  1.2943],\n",
            "         [ 1.3354, -0.5586,  0.6756,  ..., -0.5560, -0.5054,  0.1153],\n",
            "         ...,\n",
            "         [-0.3127, -1.6152, -0.1619,  ..., -0.8028,  0.1383, -0.9536],\n",
            "         [ 0.8076, -1.3337,  1.3289,  ...,  0.4579,  0.2369, -1.7600],\n",
            "         [ 0.7248, -1.1214,  0.8211,  ...,  1.7262,  0.5086,  1.5753]]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([32, 200, 512])\n",
            "attention 2 ----------\n",
            "x after first linear layer: torch.Size([32, 200, 2048])\n",
            "x after activation: torch.Size([32, 200, 2048])\n",
            "x after dropout: torch.Size([32, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([32, 200, 512])\n",
            "dropout 2 ----------\n",
            "add and norm 2 ----------\n",
            "mean size = torch.Size([32, 200, 1])\n",
            "Standard deviation : tensor([[[1.0414],\n",
            "         [1.0247],\n",
            "         [1.0223],\n",
            "         ...,\n",
            "         [1.0406],\n",
            "         [1.0391],\n",
            "         [1.0457]],\n",
            "\n",
            "        [[1.0313],\n",
            "         [1.0197],\n",
            "         [1.0326],\n",
            "         ...,\n",
            "         [1.0351],\n",
            "         [1.0012],\n",
            "         [1.0503]],\n",
            "\n",
            "        [[1.0343],\n",
            "         [1.0570],\n",
            "         [1.0333],\n",
            "         ...,\n",
            "         [1.0191],\n",
            "         [1.0295],\n",
            "         [1.0259]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.0519],\n",
            "         [1.0197],\n",
            "         [1.0307],\n",
            "         ...,\n",
            "         [1.0442],\n",
            "         [1.0287],\n",
            "         [1.0335]],\n",
            "\n",
            "        [[1.0195],\n",
            "         [1.0425],\n",
            "         [1.0283],\n",
            "         ...,\n",
            "         [1.0375],\n",
            "         [1.0119],\n",
            "         [1.0233]],\n",
            "\n",
            "        [[1.0223],\n",
            "         [1.0294],\n",
            "         [1.0311],\n",
            "         ...,\n",
            "         [1.0240],\n",
            "         [1.0368],\n",
            "         [1.0389]]], grad_fn=<SqrtBackward0>)\n",
            "y: tensor([[[ 0.1090, -0.6361,  0.1381,  ..., -1.1424,  1.9719,  0.3785],\n",
            "         [ 1.4856, -0.4747,  1.7864,  ...,  2.2660,  0.2328,  0.0518],\n",
            "         [ 1.6510, -0.6859,  0.5402,  ...,  1.4538, -0.6872, -0.2414],\n",
            "         ...,\n",
            "         [ 0.3005,  2.4590,  0.4662,  ...,  1.9996,  0.0416, -0.3032],\n",
            "         [-0.2675, -0.3966,  0.6567,  ...,  0.2209, -0.3768,  0.1933],\n",
            "         [ 1.1857, -0.7450,  0.2759,  ..., -0.2435, -0.6496,  0.0500]],\n",
            "\n",
            "        [[ 0.7054, -0.0710,  0.7140,  ..., -0.4367, -0.8751, -0.5394],\n",
            "         [ 0.6524,  1.0560, -0.2025,  ..., -0.9375,  2.0017,  1.9671],\n",
            "         [ 3.0112, -1.2370,  0.4869,  ...,  0.1143,  1.9167,  0.1232],\n",
            "         ...,\n",
            "         [ 0.2606,  1.0380, -0.2227,  ...,  0.6269,  1.4591,  1.5468],\n",
            "         [ 0.7939, -2.7190, -0.0609,  ...,  1.8241, -0.3072,  0.6777],\n",
            "         [-1.2257, -0.9172,  1.7742,  ..., -1.2239,  0.1271,  0.3292]],\n",
            "\n",
            "        [[-0.3560, -0.3164,  1.0960,  ...,  2.3509,  0.4659,  2.0578],\n",
            "         [ 1.2218, -0.9146, -0.7539,  ...,  0.3010, -0.1081,  1.4402],\n",
            "         [ 0.3241,  0.6955, -0.2570,  ...,  0.1726,  0.9946, -0.3731],\n",
            "         ...,\n",
            "         [-0.4329, -0.8296, -0.7518,  ...,  0.3134, -0.2512,  0.3235],\n",
            "         [-0.4058,  1.8862,  0.6468,  ..., -0.4375,  1.4683, -0.8864],\n",
            "         [-0.4342, -0.3799,  0.1167,  ..., -0.5165, -0.8853,  1.7011]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.3129,  1.3210,  1.3952,  ...,  1.0632, -0.1476, -1.0523],\n",
            "         [ 1.6694,  0.7840,  0.7659,  ...,  0.5461, -0.4338,  1.2058],\n",
            "         [ 0.5167,  1.5722, -0.2156,  ..., -0.2656, -0.7036,  0.5657],\n",
            "         ...,\n",
            "         [ 1.8615,  0.9120, -0.0222,  ...,  1.1557,  0.6733,  0.6016],\n",
            "         [ 0.7549,  0.6559,  0.3464,  ...,  0.4435,  1.2001,  1.9676],\n",
            "         [ 1.5365, -0.3443,  0.0047,  ..., -0.0381,  0.1721, -0.8523]],\n",
            "\n",
            "        [[ 1.1521,  0.6072,  1.7021,  ..., -0.2299, -1.2777, -0.3338],\n",
            "         [-0.0849, -0.6702,  0.2258,  ..., -0.1046, -0.5031,  0.4180],\n",
            "         [-0.7670, -0.3991, -0.7919,  ..., -0.2207, -0.4920,  0.3784],\n",
            "         ...,\n",
            "         [-0.7037, -1.5129,  0.1019,  ..., -0.6556, -1.4430,  1.6509],\n",
            "         [ 0.1801, -0.4009,  0.8233,  ..., -1.4975, -0.0700,  0.7824],\n",
            "         [-1.2239,  0.5523, -0.3754,  ...,  0.8814, -1.1891,  1.4644]],\n",
            "\n",
            "        [[ 1.7603, -0.1151,  1.2705,  ...,  0.9107,  0.5516,  0.9177],\n",
            "         [ 2.1889,  0.6608,  0.4698,  ...,  2.0747,  1.5118,  1.2517],\n",
            "         [ 1.3050, -0.7504,  0.6802,  ..., -0.4728, -0.2184,  0.1216],\n",
            "         ...,\n",
            "         [-0.3483, -1.3957, -0.1462,  ..., -0.6770, -0.0057, -1.3073],\n",
            "         [ 0.7031, -0.9573,  1.4056,  ...,  0.3983,  0.2315, -1.6946],\n",
            "         [ 0.8853, -1.3202,  0.7929,  ...,  1.3665,  0.4921,  1.6110]]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([32, 200, 512])\n",
            "Attention 1 ----------\n",
            "x.size(): torch.Size([32, 200, 512])\n",
            "qkv.size(): torch.Size([32, 200, 1536])\n",
            "qkv.size(): torch.Size([32, 200, 8, 192])\n",
            "qkv.size(): torch.Size([32, 8, 200, 192])\n",
            "q size: torch.Size([32, 8, 200, 64]), k size: torch.Size([32, 8, 200, 64]), v size: torch.Size([32, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([32, 8, 200, 200])\n",
            "values.size(): torch.Size([32, 8, 200, 64]), attention.size:torch.Size([32, 8, 200, 200]) \n",
            "values.size(): torch.Size([32, 200, 512])\n",
            "out.size(): torch.Size([32, 200, 512])\n",
            "Dropout 1 ---------- \n",
            "add and layer normalization 1 ----------\n",
            "mean size = torch.Size([32, 200, 1])\n",
            "Standard deviation : tensor([[[1.0051],\n",
            "         [1.0001],\n",
            "         [1.0081],\n",
            "         ...,\n",
            "         [0.9974],\n",
            "         [1.0020],\n",
            "         [1.0062]],\n",
            "\n",
            "        [[1.0053],\n",
            "         [1.0022],\n",
            "         [1.0024],\n",
            "         ...,\n",
            "         [0.9999],\n",
            "         [1.0003],\n",
            "         [1.0063]],\n",
            "\n",
            "        [[1.0068],\n",
            "         [1.0091],\n",
            "         [1.0037],\n",
            "         ...,\n",
            "         [1.0020],\n",
            "         [1.0018],\n",
            "         [1.0071]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.0091],\n",
            "         [1.0059],\n",
            "         [1.0046],\n",
            "         ...,\n",
            "         [1.0092],\n",
            "         [1.0077],\n",
            "         [1.0036]],\n",
            "\n",
            "        [[1.0064],\n",
            "         [0.9971],\n",
            "         [1.0058],\n",
            "         ...,\n",
            "         [1.0068],\n",
            "         [1.0027],\n",
            "         [1.0038]],\n",
            "\n",
            "        [[1.0076],\n",
            "         [1.0071],\n",
            "         [1.0042],\n",
            "         ...,\n",
            "         [0.9990],\n",
            "         [1.0014],\n",
            "         [1.0012]]], grad_fn=<SqrtBackward0>)\n",
            "y: tensor([[[ 0.2013, -0.5715,  0.0363,  ..., -0.8983,  1.9647,  0.3891],\n",
            "         [ 1.5934, -0.4128,  1.7067,  ...,  2.5179,  0.2434,  0.0365],\n",
            "         [ 1.7277, -0.5998,  0.4424,  ...,  1.6960, -0.6756, -0.2352],\n",
            "         ...,\n",
            "         [ 0.3030,  2.4491,  0.4643,  ...,  2.0958, -0.0327, -0.2324],\n",
            "         [-0.2757, -0.4089,  0.6522,  ...,  0.3174, -0.4608,  0.2503],\n",
            "         [ 1.1715, -0.7636,  0.2725,  ..., -0.1495, -0.7100,  0.1038]],\n",
            "\n",
            "        [[ 0.8105, -0.0329,  0.6097,  ..., -0.2110, -0.9214, -0.5404],\n",
            "         [ 0.7799,  1.1153, -0.2032,  ..., -0.7102,  1.9496,  1.9855],\n",
            "         [ 3.1152, -1.1663,  0.3778,  ...,  0.3373,  1.8424,  0.1181],\n",
            "         ...,\n",
            "         [ 0.2918,  0.9940, -0.1620,  ...,  0.7476,  1.3688,  1.6108],\n",
            "         [ 0.8229, -2.7663, -0.0190,  ...,  1.9453, -0.4070,  0.7169],\n",
            "         [-1.1806, -0.9553,  1.8209,  ..., -1.0892,  0.1241,  0.3779]],\n",
            "\n",
            "        [[-0.2358, -0.2460,  0.9903,  ...,  2.5111,  0.4619,  2.0440],\n",
            "         [ 1.3431, -0.8693, -0.8214,  ...,  0.4546, -0.1087,  1.4423],\n",
            "         [ 0.4626,  0.7390, -0.3399,  ...,  0.3452,  0.9384, -0.3632],\n",
            "         ...,\n",
            "         [-0.4342, -0.8552, -0.7422,  ...,  0.4249, -0.3479,  0.4152],\n",
            "         [-0.3840,  1.8639,  0.6665,  ..., -0.3234,  1.3702, -0.8868],\n",
            "         [-0.4090, -0.3803,  0.1128,  ..., -0.3961, -0.9682,  1.7958]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.3097,  1.3626,  1.2767,  ...,  1.2961, -0.1827, -1.0465],\n",
            "         [ 1.7771,  0.8428,  0.6325,  ...,  0.7646, -0.4331,  1.1969],\n",
            "         [ 0.6224,  1.6348, -0.3459,  ..., -0.0321, -0.7007,  0.5544],\n",
            "         ...,\n",
            "         [ 1.8387,  0.8352,  0.0683,  ...,  1.2533,  0.6171,  0.6140],\n",
            "         [ 0.7369,  0.5962,  0.4340,  ...,  0.5568,  1.1618,  1.9600],\n",
            "         [ 1.5318, -0.3460,  0.1031,  ...,  0.0554,  0.1215, -0.8395]],\n",
            "\n",
            "        [[ 1.2900,  0.6313,  1.6914,  ..., -0.0110, -1.3018, -0.3104],\n",
            "         [-0.0876, -0.6610,  0.1251,  ...,  0.1149, -0.5472,  0.4315],\n",
            "         [-0.6263, -0.3958, -0.8794,  ..., -0.0066, -0.5261,  0.3797],\n",
            "         ...,\n",
            "         [-0.7052, -1.5758,  0.0995,  ..., -0.5569, -1.4728,  1.6948],\n",
            "         [ 0.1792, -0.4507,  0.8826,  ..., -1.3903, -0.1013,  0.8384],\n",
            "         [-1.2209,  0.4949, -0.3166,  ...,  0.9849, -1.2365,  1.5100]],\n",
            "\n",
            "        [[ 1.8739, -0.0711,  1.2024,  ...,  0.9017,  0.5260,  0.9302],\n",
            "         [ 2.3034,  0.6921,  0.4117,  ...,  2.2709,  1.4676,  1.2661],\n",
            "         [ 1.4238, -0.6927,  0.6284,  ..., -0.2574, -0.2508,  0.1356],\n",
            "         ...,\n",
            "         [-0.3329, -1.4093, -0.1380,  ..., -0.5753, -0.0310, -1.2397],\n",
            "         [ 0.7219, -0.9676,  1.4291,  ...,  0.5082,  0.1969, -1.6250],\n",
            "         [ 0.9126, -1.3387,  0.8174,  ...,  1.4603,  0.4581,  1.6838]]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([32, 200, 512])\n",
            "attention 2 ----------\n",
            "x after first linear layer: torch.Size([32, 200, 2048])\n",
            "x after activation: torch.Size([32, 200, 2048])\n",
            "x after dropout: torch.Size([32, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([32, 200, 512])\n",
            "dropout 2 ----------\n",
            "add and norm 2 ----------\n",
            "mean size = torch.Size([32, 200, 1])\n",
            "Standard deviation : tensor([[[1.0272],\n",
            "         [1.0302],\n",
            "         [1.0437],\n",
            "         ...,\n",
            "         [1.0285],\n",
            "         [1.0242],\n",
            "         [1.0240]],\n",
            "\n",
            "        [[1.0450],\n",
            "         [1.0272],\n",
            "         [1.0453],\n",
            "         ...,\n",
            "         [1.0277],\n",
            "         [1.0111],\n",
            "         [1.0434]],\n",
            "\n",
            "        [[1.0383],\n",
            "         [1.0585],\n",
            "         [1.0395],\n",
            "         ...,\n",
            "         [1.0337],\n",
            "         [1.0354],\n",
            "         [1.0353]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.0377],\n",
            "         [1.0337],\n",
            "         [1.0445],\n",
            "         ...,\n",
            "         [1.0267],\n",
            "         [1.0201],\n",
            "         [1.0365]],\n",
            "\n",
            "        [[1.0336],\n",
            "         [1.0322],\n",
            "         [1.0251],\n",
            "         ...,\n",
            "         [1.0202],\n",
            "         [1.0355],\n",
            "         [1.0287]],\n",
            "\n",
            "        [[1.0294],\n",
            "         [1.0298],\n",
            "         [1.0202],\n",
            "         ...,\n",
            "         [1.0289],\n",
            "         [1.0254],\n",
            "         [1.0342]]], grad_fn=<SqrtBackward0>)\n",
            "y: tensor([[[ 0.2059, -0.5464,  0.1552,  ..., -0.5709,  1.6967,  0.5656],\n",
            "         [ 1.5932, -0.5643,  1.5322,  ...,  3.1401,  0.0718, -0.0562],\n",
            "         [ 1.1181, -0.6867,  0.0142,  ...,  1.8724, -0.9788, -0.2329],\n",
            "         ...,\n",
            "         [ 0.2691,  1.9662,  0.7644,  ...,  2.0638, -0.2563, -0.8466],\n",
            "         [-0.7375, -0.2693,  0.4807,  ...,  0.8345, -0.5250,  0.0392],\n",
            "         [ 0.8621, -0.7310,  0.2769,  ...,  0.1778, -0.8236, -0.2698]],\n",
            "\n",
            "        [[ 0.2530, -0.0204,  0.6482,  ...,  0.0956, -1.5303, -0.6988],\n",
            "         [ 0.4707,  0.9367, -0.4167,  ..., -0.1212,  1.8761,  1.3296],\n",
            "         [ 2.9362, -1.3065,  0.6998,  ...,  0.8818,  1.5455,  0.1190],\n",
            "         ...,\n",
            "         [-0.4895,  1.0030, -0.1908,  ...,  0.9177,  0.9307,  1.4221],\n",
            "         [ 0.8286, -2.8826, -0.1643,  ...,  2.1412, -0.6421,  0.8066],\n",
            "         [-1.3001, -1.1473,  1.7462,  ..., -1.0686,  0.3846,  0.2076]],\n",
            "\n",
            "        [[-0.3121, -0.2083,  0.8868,  ...,  2.4472,  0.2747,  1.9973],\n",
            "         [ 1.1233, -0.9631, -0.7665,  ...,  0.6859, -0.3927,  1.1365],\n",
            "         [ 0.3770,  0.7096, -0.3520,  ...,  0.8744,  0.7054, -0.7019],\n",
            "         ...,\n",
            "         [-0.9984, -0.7690, -0.5618,  ...,  1.1405, -0.3285,  0.0367],\n",
            "         [-0.5134,  2.3460,  0.3822,  ...,  0.3506,  1.4764, -1.6093],\n",
            "         [-0.6209, -0.4918,  0.0824,  ...,  0.1015, -1.0731,  1.1302]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.6234,  1.5052,  0.9406,  ...,  1.9469, -0.6429, -0.9798],\n",
            "         [ 1.0565,  0.7320,  0.5121,  ...,  1.3021, -0.3011,  1.1751],\n",
            "         [ 0.4068,  1.5135, -0.5624,  ..., -0.0181, -0.8621, -0.1818],\n",
            "         ...,\n",
            "         [ 1.3479,  0.7580,  0.0775,  ...,  1.4956,  0.6327,  0.1577],\n",
            "         [ 0.7274,  0.6366,  0.1808,  ...,  0.5430,  0.8864,  1.5871],\n",
            "         [ 1.2891, -0.5993,  0.3189,  ...,  0.2960,  0.1872, -0.7904]],\n",
            "\n",
            "        [[ 1.0127,  0.8148,  1.1831,  ...,  0.0195, -1.2742, -0.9528],\n",
            "         [-0.4563, -0.6975, -0.0904,  ...,  0.3127, -0.5115,  0.0189],\n",
            "         [-0.6141, -0.1572, -0.9061,  ...,  0.4547, -0.9466,  0.3722],\n",
            "         ...,\n",
            "         [-0.8065, -1.0761,  0.3121,  ..., -0.0049, -1.6375,  1.2345],\n",
            "         [ 0.0112, -0.4911,  0.9094,  ..., -0.9446, -0.3409,  0.7944],\n",
            "         [-1.2660,  0.6733, -0.6088,  ...,  0.9656, -1.3776,  1.4084]],\n",
            "\n",
            "        [[ 1.0623, -0.1761,  0.9982,  ...,  1.0792,  0.5297,  1.0367],\n",
            "         [ 1.8340,  0.6892,  0.4644,  ...,  2.5638,  1.6940,  0.8162],\n",
            "         [ 1.4046, -0.5669,  0.3053,  ...,  0.2829,  0.0154,  0.1418],\n",
            "         ...,\n",
            "         [-0.3550, -1.1699, -0.7272,  ..., -0.2682, -0.2484, -1.2182],\n",
            "         [ 0.7240, -0.9413,  0.9198,  ...,  0.7984,  0.4760, -1.5818],\n",
            "         [ 0.5367, -1.4615,  0.8782,  ...,  1.8648,  0.7260,  1.3989]]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([32, 200, 512])\n",
            "Attention 1 ----------\n",
            "x.size(): torch.Size([32, 200, 512])\n",
            "qkv.size(): torch.Size([32, 200, 1536])\n",
            "qkv.size(): torch.Size([32, 200, 8, 192])\n",
            "qkv.size(): torch.Size([32, 8, 200, 192])\n",
            "q size: torch.Size([32, 8, 200, 64]), k size: torch.Size([32, 8, 200, 64]), v size: torch.Size([32, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([32, 8, 200, 200])\n",
            "values.size(): torch.Size([32, 8, 200, 64]), attention.size:torch.Size([32, 8, 200, 200]) \n",
            "values.size(): torch.Size([32, 200, 512])\n",
            "out.size(): torch.Size([32, 200, 512])\n",
            "Dropout 1 ---------- \n",
            "add and layer normalization 1 ----------\n",
            "mean size = torch.Size([32, 200, 1])\n",
            "Standard deviation : tensor([[[1.0024],\n",
            "         [1.0069],\n",
            "         [1.0082],\n",
            "         ...,\n",
            "         [1.0083],\n",
            "         [1.0071],\n",
            "         [1.0032]],\n",
            "\n",
            "        [[1.0035],\n",
            "         [1.0038],\n",
            "         [1.0058],\n",
            "         ...,\n",
            "         [1.0078],\n",
            "         [1.0093],\n",
            "         [0.9990]],\n",
            "\n",
            "        [[1.0005],\n",
            "         [1.0094],\n",
            "         [1.0032],\n",
            "         ...,\n",
            "         [1.0087],\n",
            "         [1.0070],\n",
            "         [1.0062]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.0014],\n",
            "         [1.0035],\n",
            "         [1.0027],\n",
            "         ...,\n",
            "         [1.0066],\n",
            "         [1.0045],\n",
            "         [1.0010]],\n",
            "\n",
            "        [[1.0044],\n",
            "         [1.0041],\n",
            "         [0.9961],\n",
            "         ...,\n",
            "         [1.0076],\n",
            "         [1.0007],\n",
            "         [1.0037]],\n",
            "\n",
            "        [[1.0089],\n",
            "         [1.0033],\n",
            "         [0.9995],\n",
            "         ...,\n",
            "         [1.0007],\n",
            "         [1.0007],\n",
            "         [1.0023]]], grad_fn=<SqrtBackward0>)\n",
            "y: tensor([[[ 1.0964e-01, -6.0062e-01,  2.0269e-01,  ..., -6.6804e-01,\n",
            "           1.8368e+00,  5.2220e-01],\n",
            "         [ 1.4789e+00, -5.5341e-01,  1.5562e+00,  ...,  3.0099e+00,\n",
            "           7.8397e-02, -9.6004e-02],\n",
            "         [ 1.0238e+00, -6.7741e-01,  6.3131e-02,  ...,  1.8610e+00,\n",
            "          -7.9913e-01, -2.7673e-01],\n",
            "         ...,\n",
            "         [ 3.9549e-01,  1.9521e+00,  8.1296e-01,  ...,  2.0811e+00,\n",
            "          -2.5499e-01, -8.4038e-01],\n",
            "         [-5.8885e-01, -2.7929e-01,  5.5929e-01,  ...,  8.5512e-01,\n",
            "          -4.9349e-01, -1.8962e-01],\n",
            "         [ 9.9103e-01, -7.3058e-01,  2.7104e-01,  ...,  1.9700e-01,\n",
            "          -7.9225e-01, -5.0343e-01]],\n",
            "\n",
            "        [[ 1.3772e-01,  2.4553e-03,  7.3671e-01,  ..., -1.2701e-02,\n",
            "          -1.3812e+00, -6.9096e-01],\n",
            "         [ 4.7281e-01,  9.3434e-01, -3.5650e-01,  ..., -2.2334e-01,\n",
            "           2.0254e+00,  1.2270e+00],\n",
            "         [ 2.8034e+00, -1.2930e+00,  7.8075e-01,  ...,  7.6830e-01,\n",
            "           1.7035e+00,  3.0085e-02],\n",
            "         ...,\n",
            "         [-3.0667e-01,  1.0096e+00, -1.0254e-01,  ...,  9.0228e-01,\n",
            "           9.2071e-01,  1.2180e+00],\n",
            "         [ 9.9426e-01, -2.8641e+00, -8.3072e-02,  ...,  2.1223e+00,\n",
            "          -5.4592e-01,  5.9029e-01],\n",
            "         [-1.1041e+00, -1.1320e+00,  1.8268e+00,  ..., -1.0923e+00,\n",
            "           4.7470e-01, -4.3572e-03]],\n",
            "\n",
            "        [[-3.6709e-01, -2.4116e-01,  8.9050e-01,  ...,  2.3123e+00,\n",
            "           4.1126e-01,  1.9411e+00],\n",
            "         [ 1.0616e+00, -9.7683e-01, -7.5437e-01,  ...,  5.4234e-01,\n",
            "          -2.6404e-01,  1.0895e+00],\n",
            "         [ 3.2501e-01,  6.8612e-01, -3.2097e-01,  ...,  8.7914e-01,\n",
            "           8.2195e-01, -7.4961e-01],\n",
            "         ...,\n",
            "         [-7.6909e-01, -7.8191e-01, -5.1935e-01,  ...,  1.1291e+00,\n",
            "          -3.2812e-01, -2.0654e-01],\n",
            "         [-2.9741e-01,  2.3139e+00,  4.2606e-01,  ...,  3.3664e-01,\n",
            "           1.5494e+00, -1.8405e+00],\n",
            "         [-3.7450e-01, -5.1943e-01,  1.2203e-01,  ...,  1.0570e-01,\n",
            "          -1.0066e+00,  8.7969e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-6.1867e-01,  1.4507e+00,  9.6305e-01,  ...,  1.8425e+00,\n",
            "          -4.6407e-01, -1.0825e+00],\n",
            "         [ 9.6290e-01,  6.9393e-01,  5.3385e-01,  ...,  1.1874e+00,\n",
            "          -1.1660e-01,  1.0768e+00],\n",
            "         [ 3.2726e-01,  1.4629e+00, -5.4790e-01,  ..., -1.1792e-01,\n",
            "          -6.9565e-01, -2.8776e-01],\n",
            "         ...,\n",
            "         [ 1.3362e+00,  7.7897e-01,  9.8169e-02,  ...,  1.4283e+00,\n",
            "           6.4215e-01, -1.1364e-01],\n",
            "         [ 8.8061e-01,  6.6508e-01,  1.9707e-01,  ...,  5.3565e-01,\n",
            "           9.0534e-01,  1.2972e+00],\n",
            "         [ 1.4401e+00, -5.6347e-01,  3.4618e-01,  ...,  2.3865e-01,\n",
            "           1.9387e-01, -1.0481e+00]],\n",
            "\n",
            "        [[ 9.0415e-01,  7.7184e-01,  1.2122e+00,  ..., -1.2714e-01,\n",
            "          -1.0800e+00, -1.0478e+00],\n",
            "         [-5.4720e-01, -7.3386e-01, -7.1558e-02,  ...,  1.8578e-01,\n",
            "          -3.5358e-01, -7.9372e-02],\n",
            "         [-7.1571e-01, -1.9542e-01, -8.8350e-01,  ...,  4.6397e-01,\n",
            "          -7.6092e-01,  2.8325e-01],\n",
            "         ...,\n",
            "         [-8.0393e-01, -1.0619e+00,  3.8479e-01,  ...,  1.7796e-03,\n",
            "          -1.5529e+00,  1.2216e+00],\n",
            "         [ 1.9019e-01, -5.0819e-01,  9.6767e-01,  ..., -9.4587e-01,\n",
            "          -2.9264e-01,  6.0625e-01],\n",
            "         [-1.0893e+00,  6.7241e-01, -5.6623e-01,  ...,  9.6326e-01,\n",
            "          -1.3215e+00,  1.2137e+00]],\n",
            "\n",
            "        [[ 1.0129e+00, -2.5783e-01,  1.0120e+00,  ...,  1.0728e+00,\n",
            "           6.9474e-01,  1.0307e+00],\n",
            "         [ 1.7817e+00,  5.9757e-01,  4.9619e-01,  ...,  2.4243e+00,\n",
            "           1.8406e+00,  7.2771e-01],\n",
            "         [ 1.3798e+00, -6.3070e-01,  3.3644e-01,  ...,  1.4495e-01,\n",
            "           1.8078e-02,  5.1092e-02],\n",
            "         ...,\n",
            "         [-1.8114e-01, -1.1628e+00, -6.6639e-01,  ..., -2.1516e-01,\n",
            "          -2.3876e-01, -1.2210e+00],\n",
            "         [ 9.0280e-01, -9.3306e-01,  9.9816e-01,  ...,  8.3893e-01,\n",
            "           5.0302e-01, -1.8039e+00],\n",
            "         [ 7.1240e-01, -1.4487e+00,  9.4656e-01,  ...,  1.9303e+00,\n",
            "           7.4106e-01,  1.1837e+00]]], grad_fn=<DivBackward0>)\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([32, 200, 512])\n",
            "attention 2 ----------\n",
            "x after first linear layer: torch.Size([32, 200, 2048])\n",
            "x after activation: torch.Size([32, 200, 2048])\n",
            "x after dropout: torch.Size([32, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([32, 200, 512])\n",
            "dropout 2 ----------\n",
            "add and norm 2 ----------\n",
            "mean size = torch.Size([32, 200, 1])\n",
            "Standard deviation : tensor([[[1.0185],\n",
            "         [1.0318],\n",
            "         [1.0365],\n",
            "         ...,\n",
            "         [1.0271],\n",
            "         [1.0211],\n",
            "         [1.0239]],\n",
            "\n",
            "        [[1.0413],\n",
            "         [1.0674],\n",
            "         [1.0211],\n",
            "         ...,\n",
            "         [0.9961],\n",
            "         [1.0273],\n",
            "         [1.0185]],\n",
            "\n",
            "        [[1.0295],\n",
            "         [1.0083],\n",
            "         [1.0447],\n",
            "         ...,\n",
            "         [1.0399],\n",
            "         [1.0317],\n",
            "         [1.0219]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.0242],\n",
            "         [1.0378],\n",
            "         [1.0164],\n",
            "         ...,\n",
            "         [1.0118],\n",
            "         [1.0394],\n",
            "         [1.0255]],\n",
            "\n",
            "        [[1.0377],\n",
            "         [1.0300],\n",
            "         [1.0061],\n",
            "         ...,\n",
            "         [1.0311],\n",
            "         [1.0242],\n",
            "         [1.0260]],\n",
            "\n",
            "        [[1.0530],\n",
            "         [1.0244],\n",
            "         [1.0290],\n",
            "         ...,\n",
            "         [1.0210],\n",
            "         [1.0203],\n",
            "         [1.0305]]], grad_fn=<SqrtBackward0>)\n",
            "y: tensor([[[ 0.4529, -0.5902,  0.2002,  ..., -0.5880,  1.8307,  0.3289],\n",
            "         [ 1.4270, -0.1677,  1.2364,  ...,  2.8932, -0.0070, -0.0693],\n",
            "         [ 1.2745, -0.6467, -0.1834,  ...,  2.0947, -0.4927, -0.4588],\n",
            "         ...,\n",
            "         [ 0.3346,  2.3436,  1.0188,  ...,  2.0448, -0.2324, -0.2073],\n",
            "         [-0.5227, -0.2477,  0.2565,  ...,  0.8140, -0.6962, -0.1743],\n",
            "         [ 1.3493, -0.4414, -0.0073,  ..., -0.1616, -0.5758, -0.4437]],\n",
            "\n",
            "        [[ 0.4629, -0.0440,  0.7141,  ..., -0.0056, -1.3586, -0.7033],\n",
            "         [ 0.5309,  1.3244, -0.3204,  ..., -0.1058,  1.7028,  1.3227],\n",
            "         [ 2.4311, -1.4047,  0.5539,  ...,  1.1479,  2.1450,  0.0878],\n",
            "         ...,\n",
            "         [ 0.3243,  1.0159, -0.2035,  ...,  1.3555,  0.7816,  1.2250],\n",
            "         [ 0.9896, -2.8477, -0.0591,  ...,  2.1399, -0.3434,  0.8518],\n",
            "         [-0.8214, -1.2878,  1.7981,  ..., -1.0027,  0.3003,  0.5525]],\n",
            "\n",
            "        [[-0.3288, -0.0672,  0.8080,  ...,  2.5499,  0.8009,  1.9624],\n",
            "         [ 0.9015, -1.0477, -0.9493,  ...,  0.8354, -0.2807,  0.9414],\n",
            "         [ 0.3230,  0.8479, -0.2683,  ...,  1.1144,  1.0920, -0.5471],\n",
            "         ...,\n",
            "         [-0.3715, -0.7146, -0.6874,  ...,  1.1729, -0.1044,  0.0972],\n",
            "         [-0.1485,  2.3755,  0.3740,  ...,  0.3284,  2.0418, -1.4963],\n",
            "         [-0.2469, -0.2711,  0.2655,  ...,  0.1094, -1.0143,  0.8510]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.6508,  1.2588,  0.6853,  ...,  1.8911, -0.5066, -0.9901],\n",
            "         [ 1.3554,  0.6677,  0.4252,  ...,  1.3676, -0.0371,  1.1511],\n",
            "         [ 0.5907,  1.1454, -0.3526,  ...,  0.2140, -0.9263,  0.1528],\n",
            "         ...,\n",
            "         [ 1.5616,  0.6242,  0.1301,  ...,  1.8242,  0.5871,  0.2378],\n",
            "         [ 0.9704,  0.5507,  0.3121,  ...,  0.5556,  0.9641,  1.2407],\n",
            "         [ 1.3492, -0.6748,  0.4370,  ...,  0.2672,  0.5375, -1.0618]],\n",
            "\n",
            "        [[ 1.0169,  0.8813,  1.3385,  ..., -0.0202, -0.7246, -0.4651],\n",
            "         [-0.2242, -0.5130,  0.0908,  ...,  0.3214, -0.5303,  0.0896],\n",
            "         [-0.6640, -0.2693, -0.9216,  ...,  0.4638, -0.8546,  0.2842],\n",
            "         ...,\n",
            "         [-0.3409, -0.9584,  0.7946,  ...,  0.1228, -1.4469,  1.2898],\n",
            "         [ 0.7184, -0.4849,  0.7677,  ..., -0.7526, -0.0508,  0.6566],\n",
            "         [-0.7804,  0.9982, -0.6154,  ...,  0.9557, -1.6397,  1.1008]],\n",
            "\n",
            "        [[ 1.1630, -0.1034,  0.7287,  ...,  1.0670,  0.7075,  0.9863],\n",
            "         [ 1.4293,  0.6507,  0.4804,  ...,  2.3352,  1.8094,  0.9986],\n",
            "         [ 1.2126, -0.8140,  0.2105,  ..., -0.0228,  0.0811,  0.3482],\n",
            "         ...,\n",
            "         [ 0.1303, -0.9932, -0.7657,  ...,  0.1594, -0.2123, -1.3431],\n",
            "         [ 0.8829, -0.6552,  0.9764,  ...,  0.7327,  0.4849, -1.6104],\n",
            "         [ 0.7079, -1.3856,  1.2039,  ...,  1.8642,  1.0535,  1.7150]]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([32, 200, 512])\n",
            "Attention 1 ----------\n",
            "x.size(): torch.Size([32, 200, 512])\n",
            "qkv.size(): torch.Size([32, 200, 1536])\n",
            "qkv.size(): torch.Size([32, 200, 8, 192])\n",
            "qkv.size(): torch.Size([32, 8, 200, 192])\n",
            "q size: torch.Size([32, 8, 200, 64]), k size: torch.Size([32, 8, 200, 64]), v size: torch.Size([32, 8, 200, 64]), \n",
            "scaled.size() : torch.Size([32, 8, 200, 200])\n",
            "values.size(): torch.Size([32, 8, 200, 64]), attention.size:torch.Size([32, 8, 200, 200]) \n",
            "values.size(): torch.Size([32, 200, 512])\n",
            "out.size(): torch.Size([32, 200, 512])\n",
            "Dropout 1 ---------- \n",
            "add and layer normalization 1 ----------\n",
            "mean size = torch.Size([32, 200, 1])\n",
            "Standard deviation : tensor([[[1.0007],\n",
            "         [1.0005],\n",
            "         [1.0117],\n",
            "         ...,\n",
            "         [1.0033],\n",
            "         [1.0069],\n",
            "         [1.0029]],\n",
            "\n",
            "        [[0.9975],\n",
            "         [1.0036],\n",
            "         [1.0060],\n",
            "         ...,\n",
            "         [1.0069],\n",
            "         [0.9959],\n",
            "         [1.0055]],\n",
            "\n",
            "        [[1.0043],\n",
            "         [1.0065],\n",
            "         [1.0133],\n",
            "         ...,\n",
            "         [0.9931],\n",
            "         [1.0019],\n",
            "         [1.0104]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.0011],\n",
            "         [1.0082],\n",
            "         [1.0056],\n",
            "         ...,\n",
            "         [1.0089],\n",
            "         [1.0107],\n",
            "         [1.0077]],\n",
            "\n",
            "        [[1.0094],\n",
            "         [1.0087],\n",
            "         [1.0066],\n",
            "         ...,\n",
            "         [1.0016],\n",
            "         [1.0125],\n",
            "         [1.0022]],\n",
            "\n",
            "        [[1.0082],\n",
            "         [1.0060],\n",
            "         [1.0083],\n",
            "         ...,\n",
            "         [1.0053],\n",
            "         [1.0140],\n",
            "         [1.0071]]], grad_fn=<SqrtBackward0>)\n",
            "y: tensor([[[ 0.4158, -0.6173,  0.2622,  ..., -0.5872,  1.7655,  0.2530],\n",
            "         [ 1.3913, -0.1636,  1.2368,  ...,  2.9814, -0.0665, -0.1308],\n",
            "         [ 1.2469, -0.6608, -0.1305,  ...,  2.1473, -0.5499, -0.5098],\n",
            "         ...,\n",
            "         [ 0.3361,  2.2424,  1.0613,  ...,  2.0778, -0.2683, -0.2328],\n",
            "         [-0.3875, -0.3384,  0.2591,  ...,  0.8546, -0.7263, -0.2070],\n",
            "         [ 1.4695, -0.5337,  0.0626,  ..., -0.1196, -0.6239, -0.4759]],\n",
            "\n",
            "        [[ 0.4326, -0.0106,  0.7168,  ...,  0.1013, -1.3703, -0.7979],\n",
            "         [ 0.5015,  1.3453, -0.2521,  ..., -0.0077,  1.6434,  1.2352],\n",
            "         [ 2.3647, -1.3780,  0.6089,  ...,  1.2331,  2.0874,  0.0884],\n",
            "         ...,\n",
            "         [ 0.3265,  0.9668, -0.2046,  ...,  1.4053,  0.7009,  1.2112],\n",
            "         [ 1.0184, -2.9229, -0.0767,  ...,  2.1966, -0.4196,  0.8494],\n",
            "         [-0.7887, -1.3472,  1.7852,  ..., -0.9380,  0.2336,  0.5267]],\n",
            "\n",
            "        [[-0.4316, -0.0933,  0.8297,  ...,  2.6020,  0.7175,  1.8778],\n",
            "         [ 0.8169, -1.0387, -0.8890,  ...,  0.9078, -0.3562,  0.8434],\n",
            "         [ 0.2270,  0.8176, -0.2350,  ...,  1.1683,  0.9896, -0.6094],\n",
            "         ...,\n",
            "         [-0.3437, -0.7689, -0.6741,  ...,  1.1776, -0.1388,  0.0897],\n",
            "         [-0.1379,  2.2784,  0.3990,  ...,  0.3422,  1.9750, -1.5214],\n",
            "         [-0.2394, -0.3468,  0.2653,  ...,  0.1192, -1.0644,  0.8257]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.6905,  1.2872,  0.7367,  ...,  1.9547, -0.5854, -1.0799],\n",
            "         [ 1.3270,  0.7056,  0.4926,  ...,  1.4288, -0.1290,  1.0644],\n",
            "         [ 0.5893,  1.1408, -0.2941,  ...,  0.2495, -0.9951,  0.0586],\n",
            "         ...,\n",
            "         [ 1.5850,  0.5076,  0.1319,  ...,  1.8182,  0.5297,  0.1949],\n",
            "         [ 0.9925,  0.4359,  0.3128,  ...,  0.5639,  0.8909,  1.2316],\n",
            "         [ 1.3752, -0.7524,  0.4325,  ...,  0.2792,  0.5362, -1.0765]],\n",
            "\n",
            "        [[ 0.9528,  0.8694,  1.3825,  ...,  0.0450, -0.7978, -0.5562],\n",
            "         [-0.2667, -0.4890,  0.0899,  ...,  0.3745, -0.6066,  0.0043],\n",
            "         [-0.6945, -0.2504, -0.8306,  ...,  0.5156, -0.9146,  0.1971],\n",
            "         ...,\n",
            "         [-0.2743, -1.0537,  0.8429,  ...,  0.1390, -1.5056,  1.2686],\n",
            "         [ 0.7873, -0.5582,  0.8148,  ..., -0.7526, -0.1007,  0.6345],\n",
            "         [-0.7773,  0.8930, -0.5652,  ...,  0.9550, -1.7018,  1.0741]],\n",
            "\n",
            "        [[ 1.0657, -0.0718,  0.7713,  ...,  1.1189,  0.6999,  0.8922],\n",
            "         [ 1.3324,  0.6728,  0.5173,  ...,  2.3916,  1.7181,  0.9928],\n",
            "         [ 1.1194, -0.7803,  0.2672,  ..., -0.0232,  0.0106,  0.2627],\n",
            "         ...,\n",
            "         [ 0.1728, -1.0619, -0.7211,  ...,  0.1890, -0.2767, -1.3824],\n",
            "         [ 0.8756, -0.7326,  0.9727,  ...,  0.7889,  0.4081, -1.6564],\n",
            "         [ 0.7363, -1.4493,  1.2382,  ...,  1.9115,  0.9810,  1.6423]]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([32, 200, 512])\n",
            "attention 2 ----------\n",
            "x after first linear layer: torch.Size([32, 200, 2048])\n",
            "x after activation: torch.Size([32, 200, 2048])\n",
            "x after dropout: torch.Size([32, 200, 2048])\n",
            "x after 2nd linear layer: torch.Size([32, 200, 512])\n",
            "dropout 2 ----------\n",
            "add and norm 2 ----------\n",
            "mean size = torch.Size([32, 200, 1])\n",
            "Standard deviation : tensor([[[1.0450],\n",
            "         [1.0326],\n",
            "         [1.0361],\n",
            "         ...,\n",
            "         [1.0437],\n",
            "         [1.0466],\n",
            "         [1.0452]],\n",
            "\n",
            "        [[1.0307],\n",
            "         [1.0428],\n",
            "         [1.0397],\n",
            "         ...,\n",
            "         [1.0322],\n",
            "         [1.0510],\n",
            "         [1.0254]],\n",
            "\n",
            "        [[1.0236],\n",
            "         [1.0463],\n",
            "         [1.0413],\n",
            "         ...,\n",
            "         [1.0198],\n",
            "         [1.0280],\n",
            "         [1.0499]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.0351],\n",
            "         [1.0309],\n",
            "         [1.0442],\n",
            "         ...,\n",
            "         [1.0246],\n",
            "         [1.0385],\n",
            "         [1.0471]],\n",
            "\n",
            "        [[1.0443],\n",
            "         [1.0391],\n",
            "         [1.0234],\n",
            "         ...,\n",
            "         [1.0394],\n",
            "         [1.0425],\n",
            "         [1.0156]],\n",
            "\n",
            "        [[1.0457],\n",
            "         [1.0396],\n",
            "         [1.0255],\n",
            "         ...,\n",
            "         [1.0313],\n",
            "         [1.0428],\n",
            "         [1.0522]]], grad_fn=<SqrtBackward0>)\n",
            "y: tensor([[[ 4.6164e-01, -7.6570e-01, -1.1807e-01,  ..., -7.5047e-01,\n",
            "           1.4745e+00,  4.5907e-03],\n",
            "         [ 1.2223e+00,  7.7537e-02,  1.0899e+00,  ...,  2.4663e+00,\n",
            "          -7.1968e-02, -6.6045e-01],\n",
            "         [ 1.1387e+00, -6.4702e-01, -4.0141e-02,  ...,  1.9912e+00,\n",
            "          -4.8357e-01, -1.2160e+00],\n",
            "         ...,\n",
            "         [ 3.8337e-01,  1.9094e+00,  9.2071e-01,  ...,  1.5091e+00,\n",
            "          -2.4199e-01, -3.5660e-01],\n",
            "         [-2.4214e-01, -3.6068e-01, -1.6106e-01,  ...,  5.9751e-01,\n",
            "          -5.9732e-01, -3.3347e-01],\n",
            "         [ 1.4092e+00, -5.0735e-01,  1.7087e-02,  ..., -3.5820e-01,\n",
            "          -5.9362e-01, -7.1634e-01]],\n",
            "\n",
            "        [[ 4.1776e-01,  4.3898e-02,  1.3775e-01,  ...,  4.1517e-02,\n",
            "          -1.4626e+00, -7.5996e-01],\n",
            "         [ 2.2399e-01,  1.0470e+00, -7.0320e-01,  ..., -2.1955e-03,\n",
            "           1.5810e+00,  1.0396e+00],\n",
            "         [ 1.9705e+00, -1.4153e+00,  4.4339e-01,  ...,  1.4042e+00,\n",
            "           1.7297e+00,  1.4357e-03],\n",
            "         ...,\n",
            "         [ 4.9100e-01,  9.3463e-01, -2.5837e-01,  ...,  1.1712e+00,\n",
            "           6.2395e-01,  1.3698e+00],\n",
            "         [ 6.6537e-01, -3.1480e+00, -2.6578e-01,  ...,  2.1040e+00,\n",
            "          -1.7347e-01,  1.0215e+00],\n",
            "         [-7.9091e-01, -1.2526e+00,  1.5648e+00,  ..., -1.1041e+00,\n",
            "           6.6306e-01,  5.3887e-01]],\n",
            "\n",
            "        [[-5.9726e-01, -1.7308e-01,  5.2815e-01,  ...,  2.2112e+00,\n",
            "           6.9148e-01,  2.0298e+00],\n",
            "         [ 5.5249e-01, -7.2405e-01, -1.1673e+00,  ...,  7.8288e-01,\n",
            "          -7.0744e-01,  6.7346e-01],\n",
            "         [ 3.9428e-01,  7.3269e-01, -4.3351e-01,  ...,  7.5937e-01,\n",
            "           9.7287e-01, -7.9107e-01],\n",
            "         ...,\n",
            "         [-1.7217e-01, -6.1526e-01, -5.8551e-01,  ...,  8.8211e-01,\n",
            "          -1.4310e-01,  8.7262e-02],\n",
            "         [-2.8163e-01,  2.2151e+00, -4.0409e-02,  ...,  2.5090e-01,\n",
            "           1.7047e+00, -1.7911e+00],\n",
            "         [-5.1429e-01, -5.8082e-01, -3.0669e-02,  ...,  1.0115e-01,\n",
            "          -1.0262e+00,  7.4365e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-8.1177e-01,  1.2259e+00,  4.9396e-01,  ...,  2.0896e+00,\n",
            "          -4.2834e-01, -1.5044e+00],\n",
            "         [ 1.3363e+00,  3.5966e-01,  4.7449e-01,  ...,  1.3817e+00,\n",
            "           1.2855e-01,  1.0372e+00],\n",
            "         [ 5.0706e-01,  1.0381e+00, -4.8652e-01,  ...,  2.3929e-03,\n",
            "          -1.1838e+00,  2.5632e-02],\n",
            "         ...,\n",
            "         [ 1.5051e+00,  4.9875e-01, -1.3221e-01,  ...,  1.7641e+00,\n",
            "           5.6779e-01, -1.2281e-01],\n",
            "         [ 8.9345e-01,  2.1772e-01,  3.0538e-01,  ..., -4.8797e-02,\n",
            "           1.0409e+00,  9.7870e-01],\n",
            "         [ 1.4289e+00, -9.2040e-01,  2.7328e-01,  ...,  4.0474e-01,\n",
            "           5.7938e-01, -1.1633e+00]],\n",
            "\n",
            "        [[ 9.1101e-01,  6.5948e-01,  7.8815e-01,  ...,  6.5067e-02,\n",
            "          -5.0343e-01, -4.5539e-01],\n",
            "         [-4.2858e-01, -3.9081e-01,  6.9440e-02,  ...,  6.9857e-01,\n",
            "          -8.5463e-01,  2.3187e-01],\n",
            "         [-8.9856e-01, -1.6355e-01, -1.0941e+00,  ...,  6.2009e-01,\n",
            "          -1.0908e+00,  1.9358e-01],\n",
            "         ...,\n",
            "         [ 1.4574e-01, -6.9576e-01,  7.5825e-01,  ...,  1.5751e-01,\n",
            "          -1.7503e+00,  1.2087e+00],\n",
            "         [ 7.6999e-01, -5.5631e-01,  2.6903e-01,  ..., -1.1491e+00,\n",
            "           1.1790e-01,  1.9586e-01],\n",
            "         [-9.0686e-01,  1.0166e+00, -6.5855e-01,  ...,  6.1977e-01,\n",
            "          -1.5081e+00,  1.0576e+00]],\n",
            "\n",
            "        [[ 1.0647e+00, -1.8065e-02,  5.1064e-01,  ...,  1.0780e+00,\n",
            "           6.9452e-01,  8.1118e-01],\n",
            "         [ 1.0750e+00,  4.1450e-01,  1.7249e-01,  ...,  2.4784e+00,\n",
            "           1.6638e+00,  9.7268e-01],\n",
            "         [ 1.0959e+00, -8.2434e-01,  2.6493e-01,  ...,  1.1918e-01,\n",
            "           1.4623e-02,  1.5457e-01],\n",
            "         ...,\n",
            "         [ 5.2827e-01, -1.1805e+00, -7.0046e-01,  ...,  1.0353e-01,\n",
            "          -4.8479e-02, -1.4886e+00],\n",
            "         [ 1.0268e+00, -6.4702e-01,  8.3972e-01,  ...,  4.4312e-01,\n",
            "           3.7231e-01, -1.6740e+00],\n",
            "         [ 7.0206e-01, -1.0856e+00,  7.4103e-01,  ...,  1.8189e+00,\n",
            "           1.2555e+00,  1.7450e+00]]], grad_fn=<DivBackward0>)\n",
            "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
            "out: torch.Size([32, 200, 512])\n"
          ]
        }
      ]
    }
  ]
}